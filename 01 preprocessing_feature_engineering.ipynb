{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMWfj3FUkbbtttDSPWFLNRd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Resources\n","\n","https://mlcourse.ai/book/index.html\n"],"metadata":{"id":"YxFacM1m5yCW"}},{"cell_type":"markdown","source":["#Preprocessing"],"metadata":{"id":"ppEDOn-aEtlD"}},{"cell_type":"markdown","source":["## Encoding Categorical Data in Python\n","\n","1. **Label Encoding**: Converts each category into an integer value. Useful for ordinal data where the relationship between categories is meaningful.\n","\n","2. **One-Hot Encoding**: Creates binary columns for each category. Suitable for nominal data where no ordinal relationship exists.\n","\n","3. **Dummy Variable Encoding**: Similar to one-hot encoding, but reduces the number of features to avoid multicollinearity by creating N-1 features for N categories.\n","\n","4. **Binary Encoding**: Converts categories into binary digits and splits them into separate columns, balancing between one-hot and label encoding.\n","\n","5. **Frequency or Count Encoding**: Replaces categories with their counts or frequencies in the dataset. It can be useful when the frequency of a category is important.\n","\n","6. **Ordinal Encoding**: Similar to label encoding but respects the order of categories. Itâ€™s used when the categorical feature is ordinal.\n","\n","7. **Custom Mapping**: Involves defining a custom mapping based on domain knowledge, especially when the categorical data has a known order or hierarchy."],"metadata":{"id":"oCtiWDGqSQeC"}},{"cell_type":"code","source":["! pip install category_encoders\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n","import category_encoders as ce  # For Binary Encoding\n","\n","# Load Iris dataset\n","iris = load_iris()\n","df = pd.DataFrame(iris.data, columns=iris.feature_names)\n","\n","# Create a categorical feature for demonstration\n","df['flower_category'] = pd.cut(df['sepal length (cm)'], bins=3, labels=['Small', 'Medium', 'Large'])\n","print(df.head())\n","\n","\n","python\n","# One-Hot Encoding\n","one_hot_encoder = OneHotEncoder(sparse=False)\n","one_hot_encoded = one_hot_encoder.fit_transform(df[['flower_category']])\n","one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out(['flower_category']))\n","print(one_hot_df.head())\n","\n","\n","# Label Encoding\n","label_encoder = LabelEncoder()\n","df['flower_category_encoded'] = label_encoder.fit_transform(df['flower_category'])\n","print(df[['flower_category', 'flower_category_encoded']].head())\n","\n","\n","python\n","# Ordinal Encoding\n","ordinal_encoder = OrdinalEncoder()\n","df['flower_category_ordinal'] = ordinal_encoder.fit_transform(df[['flower_category']])\n","print(df[['flower_category', 'flower_category_ordinal']].head())\n","\n","\n","\n","# Binary Encoding\n","binary_encoder = ce.BinaryEncoder(cols=['flower_category'])\n","df_binary = binary_encoder.fit_transform(df['flower_category'])\n","print(df_binary.head())"],"metadata":{"id":"0tMbqQH7jVDd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","##Feature Engineering Techniques\n","\n","Feature engineering is a critical process in the preparation of data for use in machine learning. Common techniques include:\n","\n","- **Feature Transformation**: $ x' = f(x) $, where $ f $ can be log, square root, etc.\n","\n","- **Scaling** (Min-Max Scaling): $ x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} $\n","\n","- **Standardization** (Z-score Normalization): $ x' = \\frac{x - \\mu}{\\sigma} $, where $ $\\mu$ $ is the mean and $ $\\sigma$ $ is the standard deviation.\n","\n","- **Normalization**: $ x' = \\frac{x}{||x||} $, where $ ||x|| $ is the norm of the vector $ x $.\n","\n","- **Categorical Encoding** (One-hot Encoding): Convert categorical variable with $n$ categories into $n -1$ binary variables.\n","\n","- **Binning**: Partitioning continuous features into discrete intervals.\n","\n","- **Feature Creation**: $ x_{\\text{new}} = g(x_1, x_2, \\dots, x_n) $, where $ g $ is a function combining one or more existing features.\n"],"metadata":{"id":"s18y-sE-r793"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dag6yqYUouj6"},"outputs":[],"source":["from sklearn.datasets import load_boston\n","from sklearn.preprocessing import (\n","    MinMaxScaler, StandardScaler, Normalizer, FunctionTransformer, KBinsDiscretizer, PolynomialFeatures\n",")\n","from sklearn.compose import ColumnTransformer\n","import pandas as pd\n","import numpy as np\n","\n","# Load the Boston housing dataset\n","boston = load_boston()\n","X = pd.DataFrame(boston.data, columns=boston.feature_names)\n","\n","# Define transformations\n","log_transformer = FunctionTransformer(np.log1p, validate=False)\n","scaler = MinMaxScaler()\n","standardizer = StandardScaler()\n","normalizer = Normalizer(norm='l2')\n","binarizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n","polynomial = PolynomialFeatures(degree=2, include_bias=False)\n","\n","# Define the column transformer\n","pipeline = ColumnTransformer([\n","    ('log_transform', log_transformer, ['DIS', 'LSTAT']),\n","    ('scaling', scaler, ['B', 'ZN']),\n","    ('standardization', standardizer, ['CRIM', 'TAX']),\n","    ('normalization', normalizer, ['RAD', 'AGE']),\n","    ('binning', binarizer, ['INDUS']),\n","    ('polynomial_features', polynomial, ['RM', 'PTRATIO'])\n","])\n","\n","# Apply the transformations\n","X_transformed = pipeline.fit_transform(X)\n","\n","# Convert the transformed array back to a DataFrame\n","X_transformed_df = pd.DataFrame(X_transformed, columns=pipeline.get_feature_names_out())\n","\n","# Display the first few rows of the transformed DataFrame\n","print(X_transformed_df.head())"]},{"cell_type":"markdown","source":["## Class Imbalance\n","Class imbalance in datasets is a common issue in machine learning, particularly in classification problems. It arises when the number of examples in one class significantly outnumbers the examples in another, which can lead to biased models that underperform on the minority class. For instance, in a credit card fraud detection scenario, the dataset may contain 284,807 transactions, but only 492 are fraudulent. This significant imbalance requires specific handling to train an effective model.\n","\n","### Remedies for Class Imbalance\n","\n","#### 1. Downsampling\n","- **What it is**: Removing observations from the majority class.\n","- **When to use**: Preferable for large datasets (tens of thousands of samples).\n","- **Pros**: Helps to equalize class distribution, reduces the risk of overfitting.\n","- **Cons**: Can lead to loss of valuable information from the majority class.\n","\n","#### 2. Upsampling\n","- **What it is**: Increasing the number of observations in the minority class.\n","- **When to use**: Suitable for smaller datasets.\n","- **Pros**: Enhances the representation of the minority class without losing information.\n","- **Cons**: Risk of overfitting due to duplicate minority class samples or artificial noise from synthetic samples.\n","\n","#### 3. Synthetic Data Generation (e.g., SMOTE)\n","- **What it is**: Creating synthetic samples for the minority class.\n","- **Pros**: Improves model sensitivity towards the minority class without exact repetition.\n","- **Cons**: Computationally expensive and can introduce artificial noise.\n","\n","#### 4. Class Weight Adjustment\n","- **What it is**: Adjusting the model's loss function to give more weight to the minority class.\n","- **Pros**: Directly addresses imbalance within the learning algorithm without altering data.\n","- **Cons**: Requires careful tuning, not universally effective across all algorithms.\n","\n","### Evaluating Imbalanced Datasets\n","When evaluating models trained on imbalanced datasets, standard accuracy metrics can be misleading. Instead, it's crucial to use metrics like precision, recall, AUC (Area Under the Receiver Operating Characteristic curve), and AUPRC (Area Under the Precision-Recall Curve). These metrics provide a more nuanced view of the model's performance, particularly in its ability to correctly identify minority class instances.\n","\n","### Best Practices\n","- **Testing on Unaltered Data**: Regardless of the resampling method used, it's essential to evaluate the model on a test set that reflects the original class distribution. This ensures the model's performance is representative of real-world conditions.\n","- **Careful Implementation**: Manipulating class distributions can affect a model's learned class probabilities, potentially leading to over-prediction of the minority class.\n","\n","In summary, dealing with class imbalance involves choosing the right technique based on dataset size and carefully evaluating the model with appropriate metrics to ensure it performs well on both the majority and minority classes."],"metadata":{"id":"fpg4N7JOr3_y"}}]}